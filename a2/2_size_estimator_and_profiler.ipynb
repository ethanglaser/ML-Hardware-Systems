{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ML-HW-SYS/a2/blob/draft/2_size_estimator_and_profiler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Kl1A-uzhwpt_"},"source":["# **2. Model Size Estimation**\n","\n","It is no surprise that with such a tiny package, your Ardunio Nano 33 BLE Sense comes with limited memory and processing power. Therefore, you must be aware of the size and components of your model in order to have it run efficiently on your MCU.\n","\n","This notebook explores how various neural network layers affect the number of parameters, the amount memory, the number of floating point operations, and the CPU runtime of your model."]},{"cell_type":"markdown","metadata":{"id":"h5kS114ooq-0"},"source":["## 2.0 Setup GDrive and Git"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"BgbZjaQZ8niT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647139205429,"user_tz":300,"elapsed":18875,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}},"outputId":"3ff15f91-5215-4f91-87c8-3de023310420"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Mount google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"sH-xe50YtFNd","executionInfo":{"status":"ok","timestamp":1647139205803,"user_tz":300,"elapsed":387,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}}},"outputs":[],"source":["# Make sure your token is stored in a txt file at the location below.\n","# This way there is no risk that you will push it to your repo\n","# Never share your token with anyone, it is basically your github password!\n","with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n","    token = f.readline().strip()\n","# Use another file to store your github username    \n","with open('/content/gdrive/MyDrive/ece5545/user.txt') as f:\n","    handle = f.readline().strip()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"rRj2U4Y3ttgu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647139225984,"user_tz":300,"elapsed":20200,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}},"outputId":"3e413846-133b-42da-9abc-07f1d7a68a67"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/gdrive/MyDrive/ece5545’: File exists\n","/content/gdrive/MyDrive/ece5545\n","fatal: destination path 'a2-ethanglaser' already exists and is not an empty directory.\n","/content/gdrive/MyDrive/ece5545/a2-ethanglaser\n","M\t1_audio_preprocessing.ipynb\n","M\t2_size_estimator_and_profiler.ipynb\n","M\t3_training_and_analysis.ipynb\n","M\t4_model_conversion.ipynb\n","M\t5_quantization.ipynb\n","M\t6_pruning.ipynb\n","M\tsrc/train_val_test_utils.py\n","Already on 'main'\n","Your branch is up to date with 'origin/main'.\n","Already up to date.\n","/content/gdrive/MyDrive/ece5545\n"]}],"source":["# Clone your github repo\n","YOUR_TOKEN = token\n","YOUR_HANDLE = handle\n","BRANCH = \"main\"\n","\n","%mkdir /content/gdrive/MyDrive/ece5545\n","%cd /content/gdrive/MyDrive/ece5545\n","!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a2-{YOUR_HANDLE}.git\n","%cd /content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\n","!git checkout {BRANCH}\n","!git pull\n","%cd /content/gdrive/MyDrive/ece5545\n","\n","PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"R9V2uP4YtzuR","executionInfo":{"status":"ok","timestamp":1647139225985,"user_tz":300,"elapsed":18,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}}},"outputs":[],"source":["# This extension reloads all imports before running each cell\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"DIzgKXcDtFNe"},"source":["### Import code dependencies"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ePTgb55gwT7o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647139225985,"user_tz":300,"elapsed":13,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}},"outputId":"f144d7ec-bc7b-48df-84a3-762a34e45c60"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/ece5545/a2-ethanglaser\n","constants.py  networks.py\t   quant.py\n","data_proc.py  __pycache__\t   size_estimate.py\n","loaders.py    quant_conversion.py  train_val_test_utils.py\n","['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n"]}],"source":["import sys\n","print(PROJECT_ROOT)\n","!ls {PROJECT_ROOT}/src\n","print(sys.path)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2uKEHl-OtFNf","pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647139235412,"user_tz":300,"elapsed":9433,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}},"outputId":"c8f02dc9-0cd4-469c-9cd3-f05c00e9a623"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model folders are created, \n","PyTorch models will be saved in /content/gdrive/MyDrive/ece5545/models/torch_models, \n","ONNX models will be saved in /content/gdrive/MyDrive/ece5545/models/onnx_models, \n","TensorFlow Saved Models will be saved in /content/gdrive/MyDrive/ece5545/models/tf_models, \n","TensorFlow Lite models will be saved in /content/gdrive/MyDrive/ece5545/models/tflite_models, \n","TensorFlow Lite Micro models will be saved in /content/gdrive/MyDrive/ece5545/models/micro_models.\n","Imported code dependencies\n"]}],"source":["import sys,os\n","\n","# Adding assignment 2 to the system path\n","# Make sure this matches your git directory\n","sys.path.insert(0, PROJECT_ROOT)\n","\n","import torch\n","import torch.nn as nnt\n","import src.data_proc as data_proc\n","from src.constants import *\n","import numpy as np\n","\n","print(\"Imported code dependencies\")"]},{"cell_type":"markdown","metadata":{"id":"xTKG-QIfwcN9"},"source":["## 2.2 Define the Model "]},{"cell_type":"markdown","metadata":{"id":"2RRSPvUTyIQ2"},"source":["### Create the model\n","Our TinyConv model currently consists of 7 layers:\n","\n","\n","1. [Reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html)\n","2. [Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n","3. [Relu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) \n","4. [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout) \n","5. Reshape\n","6. [Fully Connected (Linear)](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n","7. [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)\n","\n","\n","Please refer to `<github_dir>/src/networks.py` for more detail."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"jU2ryBhlwcN_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647139235754,"user_tz":300,"elapsed":356,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}},"outputId":"2fc306c7-c941-48dc-c73e-e9fd03634274"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu to run the training scrpit.\n"]}],"source":["# Define device\n","from src.networks import TinyConv\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using {device} to run the training scrpit.')"]},{"cell_type":"markdown","metadata":{"id":"FLwsdGt_liKa"},"source":["### Create data_proc.AudioProcessor() object for data preprocessing\n","When an AudioProcessor instance is created: \n","\n","1. Download speech_command dataset from DATA_URL (defined in constants.py) to data_dir (default: '/content/gdrive/MyDrive/ece5545/data')\n","default dataset url: 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n","\n","2. Determine classes and their numerical indices for training and testing based on WANTED_WORDS \n","(defined in constants.py): \n","eg. if WANTED_WORDS is ['yes', 'no'], model will be trained to identify \"yes\" and \"no\" as yes and no, \n","other words as unkown, and background noises as silence\n","\n","3. Determine and save the settings for data processing feature generator based on relavent constants \n","in constants.py\n","\n","4. Determine which audio files in the dataset are for testing, training, or validating using hash method\n","\n","5. Prepare and save background noise data using the background noise data inside dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HLxcu6BB_wxg","executionInfo":{"status":"ok","timestamp":1647139364502,"user_tz":300,"elapsed":128751,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}}},"outputs":[],"source":["# Create audio processor (this takes some time the first time)\n","# And continues to run for a bit after reaching 100% while it's extracting files\n","audio_processor = data_proc.AudioProcessor()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"_RJ8a0otJEJp","executionInfo":{"status":"ok","timestamp":1647139364509,"user_tz":300,"elapsed":39,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7d2488e-6b78-450f-a151-188016f02a2b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TinyConv(\n","  (conv_reshape): Reshape(output_shape=(-1, 1, 49, 40))\n","  (conv): Conv2d(1, 8, kernel_size=(10, 8), stride=(2, 2), padding=(5, 3))\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc_reshape): Reshape(output_shape=(-1, 4000))\n","  (fc): Linear(in_features=4000, out_features=4, bias=True)\n","  (softmax): Softmax(dim=1)\n",")"]},"metadata":{},"execution_count":9}],"source":["# Create model\n","model_fp32 = TinyConv(audio_processor.model_settings)\n","model_fp32"]},{"cell_type":"markdown","metadata":{"id":"PcB7Ri8ewcOA"},"source":["## 2.3 Model Estimates\n","Run the next few cells to see how each layer impacts memory and runtime of the below TinyConv neural network model. Then experiment with reshaping it to see how adding or removing layers alters the metrics."]},{"cell_type":"markdown","metadata":{"id":"GEnwhmThwcOC"},"source":["### Memory Utilization\n","\n","There are two important forms of memory that we care about for MCUs: **flash memory** and **random access memory (RAM)**. Flash is **non-volatile** aka persistent storage memory; its data is saved when powered off. This is where your model's weights and code live, thus they must be able to fit within the capacity of your MCU's flash memory (1MB). On the other hand, RAM is **volatile** or non-persistent memory, thus it is used for temporary storage like input buffers and intermediate tensors. Together, they cannot exceed the size of your RAM storage (256KB).  "]},{"cell_type":"markdown","metadata":{"id":"f9Gv8LHUtFNi"},"source":["\n","\n","### TODO 1: Implement the `count_trainable_parameters` function in `src/size_estimate.py` to compute model size and get an estimate of the flash usage of this model\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"--J01LrjtFNi","executionInfo":{"status":"ok","timestamp":1647139364881,"user_tz":300,"elapsed":407,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3290783e-5f49-42e1-e98c-f2ab275e2a3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of trainable parameters:  0.016652 M\n"]}],"source":["# Sends model weights to the GPU if tensors are on GPU\n","if torch.cuda.is_available():\n","    model_fp32.cuda()\n","\n","from src.size_estimate import count_trainable_parameters\n","num_params = count_trainable_parameters(model_fp32)\n","print(\"Total number of trainable parameters: \", num_params / float(1e6), \"M\") # Should be about 0.016652 M"]},{"cell_type":"markdown","metadata":{"id":"-ZaeZSLrtFNi"},"source":["### TODO 2: Implement the `compute_forward_size` function in `src/size_estimate.py` to compute the memory needed for a forward pass. This is how much RAM you will be using."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"tLJ-LBfo56IL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647139365192,"user_tz":300,"elapsed":316,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}},"outputId":"9ac0549e-b235-4dcf-eb53-d99085e17af4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Forward memory:  0.007856 M\n"]}],"source":["# Sends model weights to the GPU if tensors are on GPU\n","if torch.cuda.is_available():\n","    model_fp32.cuda()\n","\n","from src.size_estimate import compute_forward_memory\n","frd_memory = compute_forward_memory(\n","    model_fp32,\n","    (1, model_fp32.model_settings['fingerprint_width'], model_fp32.model_settings['spectrogram_length']),\n","    device\n",")\n","print(\"Forward memory: \", frd_memory / float(1e6), \"M\") # Should be about 0.03462 M"]},{"cell_type":"markdown","metadata":{"id":"jkjm2OlXgArr"},"source":["As you can see above, the number of parameters in a neural network can add up fast which is a concern when dealing with a small amount of RAM. With the TinyConv neural network only consuming 0.21MB out of 1MB, our model will easily fit within flash memory. "]},{"cell_type":"markdown","metadata":{"id":"d4qkx4L8hCBD"},"source":["### Number of Operations\n","\n","### TODO 3: Implement the `flop` function in `src/size_estimate.py` to count the total FLOPS in a forward pass with batch size = 1"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Mb5ugZA3wcOD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647139365194,"user_tz":300,"elapsed":10,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}},"outputId":"0de98b7a-ca3c-4617-b2bd-34863e1a9c0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["total number of floating operations: 1316004\n","Number of FLOPs by layer and parameters:\n","Conv:  {'bias': 4000, 'weight': 1280000}\n","FC:    {'bias': 4, 'weight': 32000}\n"]}],"source":["from pprint import pprint\n","from src.size_estimate import flop\n","\n","if torch.cuda.is_available():\n","    model_fp32.cuda()\n","\n","# The total number of floating point operations \n","flop_by_layers = flop(\n","    model=model_fp32, \n","    input_shape=(\n","        1, \n","        model_fp32.model_settings['fingerprint_width'], \n","        model_fp32.model_settings['spectrogram_length']\n","    ), \n","    device=device)\n","total_param_flops = sum([sum(val.values()) for val in flop_by_layers.values()])\n","\n","\n","print(f'total number of floating operations: {total_param_flops}')  # total number of floating operations: 340004\n","print('Number of FLOPs by layer and parameters:') \n","print(\"Conv: \", flop_by_layers['conv'])  # {'bias': 4000, 'weight': 320000}\n","print(\"FC:   \", flop_by_layers['fc'])  # {'bias': 4, 'weight': 16000}"]},{"cell_type":"markdown","metadata":{"id":"wbAbpr4Mj12i"},"source":["### CPU runtime\n","\n","### TODO 4: Measure the server/desktop CPU runtime to compare to the MCU runtime later in this assignment"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"zMcAtzKHwcOD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647139365346,"user_tz":300,"elapsed":159,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}},"outputId":"d7971c98-c49a-42aa-f715-1e4ed3e4e3c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n","----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","             model_inference        19.42%     752.000us        94.42%       3.657ms       3.657ms             1            --  \n","                aten::conv2d         1.32%      51.000us        54.89%       2.126ms       2.126ms             1       640.000  \n","           aten::convolution         1.19%      46.000us        53.58%       2.075ms       2.075ms             1            --  \n","          aten::_convolution         1.50%      58.000us        52.39%       2.029ms       2.029ms             1            --  \n","    aten::mkldnn_convolution        50.68%       1.963ms        50.89%       1.971ms       1.971ms             1            --  \n","                aten::linear         2.40%      93.000us        11.49%     445.000us     445.000us             1            --  \n","                 aten::addmm         5.55%     215.000us         6.82%     264.000us     264.000us             1        32.000  \n","                 aten::zeros         5.24%     203.000us         5.58%     216.000us     216.000us             1            --  \n","                  aten::relu         1.47%      57.000us         4.13%     160.000us     160.000us             1            --  \n","             aten::clamp_min         2.58%     100.000us         3.23%     125.000us      62.500us             2            --  \n","               aten::reshape         2.17%      84.000us         2.79%     108.000us      54.000us             2            --  \n","                     aten::t         1.27%      49.000us         2.27%      88.000us      88.000us             1            --  \n","               aten::softmax         1.14%      44.000us         1.60%      62.000us      62.000us             1            --  \n","             aten::transpose         0.96%      37.000us         1.01%      39.000us      39.000us             1            --  \n","                aten::expand         0.93%      36.000us         0.98%      38.000us      38.000us             1            --  \n","        aten::_reshape_alias         0.62%      24.000us         0.62%      24.000us      12.000us             2            --  \n","                 aten::empty         0.54%      21.000us         0.54%      21.000us       5.250us             4            --  \n","              aten::_softmax         0.46%      18.000us         0.46%      18.000us      18.000us             1            --  \n","                 aten::copy_         0.26%      10.000us         0.26%      10.000us      10.000us             1            --  \n","           aten::as_strided_         0.10%       4.000us         0.10%       4.000us       4.000us             1            --  \n","----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 3.873ms\n","\n"]}],"source":["model_fp32.cpu()\n","model_fp32.eval()\n","inputs = torch.rand([1,1960]).cpu()\n","\n","# Run a profiler to see the cpu time for inference \n","from torch.profiler import profile, record_function, ProfilerActivity\n","with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops=True, with_stack=True) as prof:\n","    with record_function(\"model_inference\"):\n","        model_fp32(inputs)\n","print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"]},{"cell_type":"code","source":[""],"metadata":{"id":"GyvopFz_HFio","executionInfo":{"status":"ok","timestamp":1647139365347,"user_tz":300,"elapsed":4,"user":{"displayName":"Ethan Glaser","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09497163345803200296"}}},"execution_count":13,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"2_size_estimator_and_profiler.ipynb","provenance":[]},"interpreter":{"hash":"92bf126df007708fd70c442c808ee74575bedf7ea6317e0b182c3af0184af25d"},"kernelspec":{"display_name":"ece5545","language":"python","name":"ece5545"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":0}